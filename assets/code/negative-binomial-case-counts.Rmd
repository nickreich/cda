---
title: Using a Negative Binomial model with smooth splines to predict infectious disease
  case counts
author: "Nick Reich, adapted from work by Steve Lauer et al"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(ggplot2)
library(dplyr)
library(readr)
theme_set(theme_minimal())
```

## Introduction

This analysis is based on research that is [publicly available on GitHub](https://github.com/reichlab/annual-predictions-paper), and was [published in PNAS in 2018](https://www.pnas.org/doi/10.1073/pnas.1714457115). The code and description have been adapted for this write-up. Here is are some expository excerpts from the paper:

> Dengue, a mosquito-borne virus prevalent throughout the tropics and sub-tropics, infects an estimated 390 million people every year.
While the majority of infections are mild or asymptomatic, the more severe forms of dengue infection -- dengue shock syndrome (DSS) and dengue hemorrhagic fever (DHF) -- can result in organ failure or death.


> ... In Thailand, dengue infection is endemic with substantial annual and geographic variation in incidence across its 76 provinces and 13 health regions. Over the past 15 years, an average of 43,137 (range 14,952-106,320) DHF cases have been reported to the Thailand Ministry of Public Health (MOPH) each year.
Within a typical year, incidence rates in different provinces can vary by an order of magnitude, with some provinces experiencing less than 10 DHF cases per 100,000 population and others over 100 per 100,000 population.

> ... Whether an infectious disease spreads within a population depends on the transmission rate of the disease and the number of susceptible individuals, thus long-term forecasting models for DHF incidence may need to account for climatic factors that could affect transmission as well as population susceptibility.
Climatic factors, such as temperature, rainfall, and humidity, may impact both the prevalence and distribution of the dengue vector, the \textit{Aedes} mosquito, as well as the transmission efficiency of dengue virus.
Such climatic factors vary significantly across Thailand.

> ... We obtained data on DHF cases (from the MOPH), population (National Statistical Office of Thailand), and weather (NOAA).
These data were summarized across time frames ranging from one month to one year to create covariates for consideration by our model selection algorithm. ...
We made forecasts using the data available in April of each year, the month when the MOPH has historically finalized the incidence reports obtained from all provinces for the prior calendar year.
Hence, all ``annual'' forecasts are for DHF incidence between April and December of the year they are made.

> ... For the NOAA and NCDC weather station data, we found the most consistently reported weather station for each province and extracted the daily maximum and minimum temperature, maximum humidity, and rainfall.
We aggregated these measures into monthly covariates for maximum, minimum, and mean temperature, maximum and mean humidity, and maximum and total rainfall across the low-season, January, February, and March.



## The data

The dataset shown here and provided in this repository is the same one used in the analysis. Some columns have been removed for simplicity. 

The dataset is available as a [CSV file](https://github.com/nickreich/cda/blob/master/assets/code/dengue-data.csv). A [codebook for the data](https://github.com/nickreich/cda/blob/master/assets/code/dengue-data-codebook) is also available.

In this analysis, we split the dataset into a training dataset from 2000 through 2012 and a testing dataset from 2013 and 2014.

```{r}
dat <- read_csv("https://raw.githubusercontent.com/nickreich/cda/master/assets/code/dengue-data.csv")
train_dat <- dat %>%
  dplyr::filter(year<2013) %>% ## all but 2013 and 2014
  mutate(pid = factor(pid)) ## pid needs to be a factor for random effects
test_dat <- dat %>%
  dplyr::filter(year>=2013) %>%
  mutate(pid = factor(pid))
str(train_dat)
```


```{r}
ggplot(dat, aes(x=year, y=reorder(pid, population), fill=obs/population)) + geom_tile() + 
    scale_fill_gradient(low="grey100", high="firebrick") + 
    ylab("Province (by population, large at top)") + xlab(NULL) 
```

Before we begin modeling, let's see if there is evidence that we will need to compensate for overdispersion in a Poisson model.

```{r}
disp_dat <- train_dat %>% 
    group_by(pid) %>%
    summarize(
        avg_inc_rate = mean(obs),
        var_inc_rate = var(obs),
        var_over_avg = var_inc_rate/avg_inc_rate
    )
quantile(disp_dat$var_over_avg)
```

There sure is. The per-province variance ranges from 7 to over 1500 times the accompanying mean.


## Modeling

### Negative binomial model

> The model that we used to forecast annual DHF incidence for this study is a generalized additive model \cite{Hastie2009}.
Specifically, we use a generalized additive model with a negative binomial family, separate penalized smoothing splines for each covariate, and province-level random effects:

$$    Y_{i,t} \sim \text{NB}(n_{i,t}\lambda_{i,t}, r) $$
$$    \log \Big [ \mathbf{E}(Y_{i,t}) \Big ] = \beta_0 + \log(n_{i,t}) + \alpha_i + \sum_{j=1}^J g_j(x_{j,i,t}|\boldsymbol{\theta}) $$
$$     \alpha_i \sim \text{Normal}(\mu, \sigma^2) $$

> We model the incidence ($Y_{i,t}$) for province $i$ in year $t$ as following a negative binomial distribution with the mean equal to the province population ($n_{i,t}$) times the incidence rate ($\lambda_{i,t}$) and a dispersion parameter $r$.
After a log transformation, we model the mean of this distribution using an intercept ($\beta_0$), a random effect for each province ($\alpha_i$) and a cubic spline for each of $J$ covariates ($g_j(x_{j,i,t}|\boldsymbol{\theta})$).

Let's fit a model and look at the results.
```{r}
library(mgcv)
fmla <- formula("obs ~ offset(log(population)) + 
                s(pid, bs = 're') +
                s(pre_season_rate, k = 3, m=2, bs = 'cr') +
                s(Jan_temp_grid, k = 3, m=2, bs = 'cr') +
                s(last_high_rate, k = 3, m=2, bs = 'cr')")
mod_nb <- gam(fmla, data = train_dat, family = nb())
summary(mod_nb)
```

The random effects term is significant but notice that the degrees of freedom is fairly high relative to the number of provinces (76), indicating quite a bit of variation between the provinces and not a lot of shrinkage or pooling of information.
It also looks like all of our chosen covariates are "highly significant", but because these relationships are modeled as smooth functions of the covariates, there are no coefficients to interpret. What do these relationships look like?

```{r}
par(mfrow=c(2,2))
plot(mod_nb)
```

We can also fit some reduced models to compare deviance:
```{r}
mod_ref <- gam(obs ~ offset(log(population))+1, data = train_dat, family = nb)
mod_randint <- gam(obs ~ offset(log(population)) + s(pid, bs = 're'), data = train_dat, family = nb)
anova(mod_ref, mod_randint, mod_nb)
```

And here is a Poisson model for comparison. It's interesting to see how the Poisson model has quite a bit less uncertainty in the estimated relationships.

```{r}
mod_pois <- gam(fmla, data = train_dat, family = poisson)
par(mfrow=c(2,2))
plot(mod_pois)
```


## Obtaining predictive distribution

Originally, this model was fit with the primary goal of predicting incidence rather than describing relationships between variables. This of course serves the purpose of doing "predictive model checking". In particular, we wanted to obtain predictive distributions of incidence. This turned out to be not a straight-forward process.

> To obtain predictive distribution samples, we use a two-stage procedure to incorporate the uncertainty from our model parameter estimates and from the negative binomial distribution.
We first draw 100 sample parameter sets from a multivariate normal distribution with mean equal to the point estimates of the parameters ($\boldsymbol{\theta}, \mu, \sigma^2$) from Equations (\ref{eqn:regression})-(\ref{eqn:RE}) and covariance equal to the matrix of standard errors.
Each of these sampled parameter sets yields a corresponding $\widehat{\lambda}_{i,t}$.
We then draw 100 samples from the negative binomial distribution given in Equation (\ref{eqn:NB}) for each $\widehat{\lambda}_{i,t}$ with the fixed estimate of $r$ to obtain a sample of size 10,000 from the predictive distribution for $Y_{i,t}$.
We calculate the point estimate for each province-year, $\widehat Y_{i,t}$, as the median of these samples from the predictive distribution.
The lower and upper limits of the 80\% prediction intervals were defined by taking the 10$^{th}$ and 90$^{th}$ percentiles of these samples from the predictive distribution.

```{r}
#' Prediction intervals for negative binomial GAM regression
#' Randomly samples the multivariate normal distribution of the coefficient parameters and draws from a negative binomial distribution to create prediction intervals for a GAM spline of the negative binomial family.
#'
#'
#' @param train_fit gam object from mgcv's gam(), conducted with family = nb()
#' @param test_dat data to make predictions on
#' @param coef_perms number of normally distributed coefficient permutations
#' @param nb_draws number of draws from a negative binomial for each coefficient permutation
pred_dist <- function(train_fit,
                      test_dat,
                      coef_perms,
                      nb_draws){
    require(mgcv)
    if(missing(nb_draws))
        nb_draws <- coef_perms
    ## get the mean and covariance matrix for predictions on the testing data
    test_preds <- predict(train_fit, test_dat, type = "lpmatrix")
    ## replicate parameter vectors
    rep_params <- rmvn(coef_perms,coef(train_fit),train_fit$Vp)
    pred_count <- matrix(0, nrow=nrow(test_dat), ncol=coef_perms)
    ## replicate predictions for each set of parameters
    for (i in 1:coef_perms) {
        rep_preds <- test_preds %*% rep_params[i,]
        pred_count[,i] <- exp(rep_preds)*test_dat$population
    }
    ## find the dispersion parameter for the negative binomial
    r <- train_fit$family$getTheta(trans=TRUE)
    ## sample from the negative binomial distribution
    all_preds <- matrix(rnbinom(nrow(pred_count)*coef_perms*nb_draws,
                                mu=as.vector(pred_count), size=r),
                        nrow=nrow(test_dat)) %>%
        as.data.frame()
    return(all_preds)
}
preds <- pred_dist(train_fit=mod_nb,
    test_dat=test_dat,
    coef_perms=100,
    nb_draws=10)

test_dat$point_est <- apply(preds, MAR=1, FUN=median)
test_dat$ci_80_lb <- apply(preds, MAR=1, FUN=function(x) quantile(x, .1))
test_dat$ci_80_ub <- apply(preds, MAR=1, FUN=function(x) quantile(x, .9))
test_dat$pid <- reorder(test_dat$pid, test_dat$population)

# prediction_dat <- data_frame(year = test_dat$year,
#     pid = test_dat$pid,
#     obs_counts = test_dat$obs) %>%
#     bind_cols(preds) %>%
#     gather("sim", "pred_counts", starts_with("V")) %>%
#     filter(!is.na(pred_counts))

ggplot(test_dat, aes(x=pid)) + 
    geom_segment(aes(xend=pid, y=ci_80_lb, yend=ci_80_ub), alpha=.5, size=1) +
    geom_point(aes(y=point_est)) + 
    geom_point(aes(y=obs), color="red", shape="x") +
    scale_y_log10() + ylab("incidence") +
    xlab("province (by population size)") + scale_x_discrete(label=NULL) +
    theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
    facet_grid(.~year)

```

Above, the predicted incidence is in black (with 80% prediction interval) and the red dots are the observed data. In the paper itself, there are similar graphics with comparisons to a simple model that just predicts the average incidence in a given year.

The 80% coverage for the two test seasons can be calculated as:
```{r}
test_dat$coverage <- with(test_dat, (obs >= ci_80_lb) & (obs <= ci_80_ub))
sum(test_dat$coverage)/nrow(test_dat)
```


