---
title: 'Lecture 6: Likelihood Ratio Confidence Intervals & Bayesian Methods for Contingency
  Tables'
author: "Nick Reich / transcribed by Bianca Doone and Guandong Yang"
output:
  beamer_presentation:
    includes:
      in_header: ../../slide-includes/beamer-header-for-pandoc.tex
    keep_tex: yes
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MCMCpack)
library(ggtern)
library(dplyr)
library(tidyr)
library(ggplot2)
```

 Likelihood Ratio Confidence Intervals
========================================================
\textbf{Motivating Example}: Poisson Sample-Mean Estimation (1-parameter Poisson Mean)

- The probability mass function (pmf) for the Poisson distribution is defined as:

\begin{center}

$p(y|\mu) = \frac{e^{-\mu}\mu^y}{y!}$, $y=0,1,...$ (non-negative integers)

with $\mathbb{E}(Y) = \mathbb{V}(Y) = \mu$

\end{center}

- Given observations $y_1, y_2,..., y_n$, assuming $y_i \stackrel{iid}{\sim}\mbox{Poisson}(\mu)$, the log-likelihood is defined as:

\begin{center}
$$L(\mu|\mathbf{y}) = \log \left(\prod_{i=1}^n \frac{e^{-\mu}\mu^{y_i}}{y_i!} \right) = \log \left(\frac{e^{-n\mu}\mu^{\sum_{i=1}^n y_i}}{\prod_{i=1}^n y_i!} \right)$$
$$= -n\mu + \log(\mu)\sum_{i=1}^n y_i + \mbox{C}$$
where $\mbox{C} = \prod_{i=1}^n y_i!$ is a constant.

\end{center}

 LR CI's - Motivating Example Cont'd
========================================================
\begin{center}

$$L(\mu|\mathbf{y}) \propto -n\mu + \log(\mu)\sum_{i=1}^n y_i$$
\end{center}

- Note that taking the first derivative and setting equal to zero provides us the MLE for the data: 

$$\hat{\mu} = \bar{y} = \frac{1}{n}\sum_{i=1}^n y_i$$

- Let $\tilde{y} = (2,2,3)^T$. Then the log-likelihood is


$$L(\mu|\tilde{y}) \propto -3\mu + 7 \log(\mu)$$
maximized at $\hat{\mu} = 7/3$.



 LR CI's - Visualization of this likelihood
========================================================

```{r, echo= FALSE}
l_mu = function(x){-3*x + 7*log(x)}
plot(y = l_mu(c(seq(0,8, by = 0.01))), x = seq(0,8, by = 0.01), type="l", ylab = "Log Likelihood", xlab = "mu" )
abline(v = 7/3, lty = 2)
```


 LR CI's - A Few Notes
========================================================

- Each point on this curve represents a "fit" to the data.  
- In general, adding more data implies that the likelihood is going to be lower.  
- Likelihoods always need to be relative (i.e. $L_1$ vs. $L_0$).  
- Using an absolute scale is not particularly meaningful. 
    


 $(1-\alpha)\%$ likelihood-based C.I.
========================================================

Suppose $\Theta$ is a set of parameters, and we let $\Theta$ or a subset of $\Theta$ vary

\textbf{Heuristic Definition}:  

- We are interested in the set $\Theta$ for which
$$\mbox{LRTS}(\Theta) = -2[L(\Theta) - L(\hat{\Theta})] < \chi^2_{df}(1-\alpha)$$
with $\hat{\Theta}$ as the fixed value at the MLE, and the LR test statistic compared to the $(1-\alpha)^{\mbox{th}}$ quantile of $\chi^2_{df}$  

- The set of $\Theta$ where this holds is a $(1-\alpha)\%$ confidence interval with degrees of freedom equal to the number of parameters the likelihood is varying over (free parameters)


 LR CI's - Motivating Example Cont'd
========================================================
\textbf{Motivating Example}: Poisson Sample-Mean Estimation (1-parameter Poisson Mean)  

- Let $\Theta = \mu$, then we can define:
$$\mbox{LRTS}(\mu) = -2[L(\mu) - L(\hat{\mu})]$$

- Setting $\alpha = 0.05$, we can define a $95\%$ confidence interval for $\mu$ as
$$\left\{\mu: \mbox{LRTS}(\mu) < \chi^2_1(.95) \right\}$$
$$= \left\{\mu: -2[L(\mu)-L(\hat{\mu})] < \chi^2_1(.95) \right\}$$
$$= \left\{\mu: L(\mu) > L(\hat{\mu}) - \frac{\chi^2_1(.95)}{2} \right\}$$

 LR CI's - Motivating Example Cont'd
========================================================
Using the MLE, $\hat{\mu} = 7/3$, and the value of the $\chi^2_1(.95) = 3.84$, we derive an approximate $95\%$ confidence interval for $\mu$:
$$\left\{\mu: L(\mu) > L(7/3) - \frac{3.84}{2} \right\} \approx \left\{\mu: 1.1 \le \mu \le 4.5 \right\}$$
```{r, echo = F}
plot(y = l_mu(c(seq(0,8, by = 0.01))), x = seq(0,8, by = 0.01), type="l", ylab = "Log Likelihood", xlab = "mu", ylim = c(-5,0) )
abline(v = 7/3, lty = 2)
abline(h = -1.068915 -3.84/2, lty = 3, col = "red")
abline(h = -1.068915, lty = 3, col = "red")
text(x = 6, y = -2.8, labels = "L = L(mu) - Z/2", col = "red")
text(x = 6, y = -0.9, labels = "L = L(mu)", col = "red")
```




 Bayesian Method for Contingency Tables
========================================================

\begin{center}
\textit{
Bayesian methods for contingency tables may be a good alternative to small sample-size methods because there is less reliance on large sample theory...but could be sensitive to \textbf{prior choice}.
}
\end{center}

- Supplement on impact of priors: See Chapter 3.6.4 in Agresti.


 Beta/Binomial Example - Handedness
========================================================

\textbf{Q: Are women and men left-handed at the same rate?}

\begin{center}
\begin{tabular}{|r|c|c|c|}\hline
Gender & RH & LH & Total \\\hline
Men & 43 & 9 & $n_1 = 52$ \\\hline
Women & 44 & 4 & $n_2 = 48$ \\\hline
Total & 77 & 13 & 100 \\\hline
\end{tabular}
\end{center}

In other words:

- Is there a difference in the proportions of men who are left-handed and women who are left-handed?
$$H_0: \mbox{Pr}(\mbox{left-handed}|\mbox{male}) = \mbox{Pr}(\mbox{left-handed}|\mbox{female})$$

- Is the difference between left-handed men and left-handed women equal to zero?
$$H_0: \mbox{Pr}(\mbox{left-handed}|\mbox{male}) - \mbox{Pr}(\mbox{left-handed}|\mbox{female}) = 0$$

 Beta/Binomial Example - Handedness
========================================================

When we have $2 \times 2$ table, the chi-square test for independence is equal to two-sided test for different proportion.

```{r}
dat <- matrix(c(43, 9 ,44, 4), ncol =2, byrow = T)
chi <- chisq.test(dat, correct = F)
dif <- prop.test(dat, correct = F)
```

```{r echo = F}
chi.stat <- c("Chi-square test", round(chi$statistic,3), chi$parameter, round(chi$p.value,3))
dif.stat <- c("Difference Two proportion", round(dif$statistic,3), dif$parameter, round(dif$p.value,3))
summary_table <- data.frame(rbind(chi.stat, dif.stat))
names(summary_table) <- c("Tests", "Test Statistics", "DF", "P-vlaue")
rownames(summary_table) <- NULL
knitr::kable(summary_table)
```


 Beta/Binomial Example - Handedness
========================================================

- \textbf{Probability Structure}:

    * Men who are left-handed: $Y_1 \sim \mbox{Bin}(n_1, \pi_1)$

    * Women who are left-handed: $Y_2 \sim \mbox{Bin}(n_2, \pi_2)$

- \textbf{Observed Data}:

    * $(y_1,y_2) = (9,4)$
    * $(n_1,n_2) = (52,48)$

- Let us assign a Uniform prior onto $\pi_1$ and $\pi_2$, such that

    * $\pi_1 \sim U(0,1)$ (also considered $\sim \mbox{Beta}(1,1)$)
    * $\pi_2 \sim U(0,1)$

- Because the Beta distribution is a \textit{conjugate prior} to the Binomial likelihood, the  \textbf{posterior} distribution for $pi_1$ and $\pi_2$ is
$$p(\pi_1|y,n) \sim \mbox{Beta}(y_1 + 1, n_1 - y_1 + 1)$$
$$p(\pi_2|y,n) \sim \mbox{Beta}(y_2 + 1, n_2 - y_2 + 1)$$

 Bayesian Method - Computational Technique
========================================================
1. Simulate N independent draws from $p(\pi_1|y,n)$ and $p(\pi_2|y,n)$  

2. Compute $\theta_i, i = 1,...,N$  

3. Plot empirical posterior  

4. Calculate summary statistics  



 Bayesian Method - Multinomial/Dirichlet
========================================================

- Suppose $y$ is a vector of counts with number of observations for each possible outcome, $j$
- Then, the likelihood can be written as
$$p(y|\theta) \propto \prod_{j=1}^k \theta_j^{y_i}$$
where $\sum_j \theta_j = 1$ and $\theta$ is a vector of probabilities for $j$.

- The conjugate prior distribution is a multivariate generalization of the Beta distribution: \textbf{The Dirichlet}

 Bayesian Method - Multinomial/Dirichlet
========================================================
- We set the Dirichlet distribution as the prior for $\theta$: $\theta \sim \mbox{Dir}(\alpha)$, with pdf:
$$p(\theta|\alpha) \propto \prod_{j=1}^k \theta_j^{\alpha_j -1}$$
where $\alpha$ is a hyper parameter, and $\theta_j > 0, \sum_j \theta_j = 1$
- The posterior distribution can then be derived as
$$p(\theta|y) \sim \mbox{Dir}\left(\begin{array}{c}\alpha_1 + y_1 \\ \alpha_2 + y_2 \\
\vdots \\ \alpha_k + y_k \end{array}\right)$$

- Plausible "non-informative" priors
    * Set $\alpha_j = 1, \forall j$ gives equal density to any vector $\theta$ such that $\sum_j \theta_j = 1$
    * Set $\alpha_j = 0, \forall j$ (improper prior) gives a uniform distribution in $\log(\theta_j)$ (if $y_i > 0, \forall j$, we have a proper posterior)


 Multinomial/Dirichlet - Example in R
========================================================
\textit{Adapted from Bayesian Data Analysis 3}

- A poll was conducted with $n=1447$ participants, with the following results:
    * Obama: $y_1 = 727$
    * Romney: $y_2 = 583$
    * Other: $y_3 = 137$
    
- The estimand of interest is $\theta_1 - \theta_2$
- Assuming simple random sampling, we have
$$(y_1,y_2,y_3) \sim \mbox{Multinomial}(n, \left(\begin{array}{c} \theta_1 \\ \theta_2 \\ \theta_3 \end{array} \right))$$
- We now apply the same computational technique as in the univariate case...

 Multinomial/Dirichlet - Example in R
========================================================
```{r}
#data
y <- c(727, 583, 137)

#"uniform" hyperparameter
a <- c(1,1,1)

#prior
pri <- rdirichlet(100, a)

#Generate Posterior
postr <- rdirichlet(1000, y+a)
```


 Multinomial/Dirichlet - Visualization of Prior
========================================================
```{r echo= F}
labels <- c(V1 = "Group 1", V2= "Group 2", V3 = "Group 3")
main <- "Histogram of difference between group 1 and group 2"
```

```{r, echo=FALSE}
#Visualization of Prior
as_data_frame(pri) %>% 
    gather() %>%
    ggplot(aes(x=value)) +  geom_density() + 
  facet_grid(key~., labeller=labeller(key = labels))+ xlim(0,1)
```


 Multinomial/Dirichlet - Visualization of Prior (3D)
========================================================
```{r, echo=FALSE}
as_data_frame(pri) %>%
    ggplot(aes(V1, V2, V3)) +
    coord_tern() +
    geom_point()
```

 Multinomial/Dirichlet - Visualization of Posterior
========================================================
```{r, echo=FALSE}
#Visualization of Posterior
as_data_frame(postr) %>% 
    gather() %>%
    ggplot(aes(x=value)) +  geom_density() + 
  facet_grid(key~., labeller=labeller(key = labels)) + xlim(0,1)
```

 Multinomial/Dirichlet - Visualization of Posterior (3D)
========================================================
```{r, echo=FALSE}
as_data_frame(postr) %>%
    ggplot(aes(V1, V2, V3)) +
    coord_tern() +
    geom_point()
```

 Multinomial/Dirichlet - Summary Statistics
========================================================
```{r}
poll_diff <- postr[,1]-postr[,2]
hist(poll_diff, main = main)
```

 Multinomial/Dirichlet - Summary Statistics
========================================================
```{r}
### Point Estimates
mean(poll_diff)
### P-value
mean(poll_diff >0)
### 95% CI
quantile(poll_diff, c(.025, .975))
```









